{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "#### Name:Pranav\n",
    "#### Regno:2347137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Twitter Airline\n",
    "\n",
    "Design a sentiment analysis classification model using backpropagation and activation functions\n",
    "such as sigmoid, ReLU, or tanh. Implement a neural network that can classify sentiment\n",
    "(positive/negative) from a small dataset. Demonstrate how backpropagation updates the weights\n",
    "during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple feed-forward neural network for binary sentiment classification\n",
    "(positive/negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"D:/@5trisem/NN/Tweets.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "X = data['text']\n",
    "y = data['airline_sentiment']\n",
    "\n",
    "# Keep only positive and negative sentiments for binary classification\n",
    "data = data[data['airline_sentiment'] != 'neutral']\n",
    "\n",
    "# Encode labels: 1 for positive, 0 for negative\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Use TfidfVectorizer for better performance over CountVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating A neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights with better scaling\n",
    "        self.weights_ih = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0/self.input_size)\n",
    "        self.bias_h = np.zeros((1, self.hidden_size))\n",
    "        self.weights_ho = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0/self.hidden_size)\n",
    "        self.bias_o = np.zeros((1, self.output_size))\n",
    "        \n",
    "        # Set the activation function for hidden layer\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.hidden = self.activation(np.dot(X, self.weights_ih) + self.bias_h)\n",
    "        self.output = sigmoid(np.dot(self.hidden, self.weights_ho) + self.bias_o)  # Always use sigmoid for output layer\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Calculate the error\n",
    "        self.error = y - output\n",
    "        d_output = self.error * sigmoid_derivative(output)  # Sigmoid derivative for output layer\n",
    "        \n",
    "        # Backpropagate the error to the hidden layer\n",
    "        error_hidden = np.dot(d_output, self.weights_ho.T)\n",
    "        d_hidden = error_hidden * self.activation_derivative(self.hidden)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_ho += learning_rate * np.dot(self.hidden.T, d_output)\n",
    "        self.bias_o += learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.weights_ih += learning_rate * np.dot(X.T, d_hidden)\n",
    "        self.bias_h += learning_rate * np.sum(d_hidden, axis=0, keepdims=True)\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            loss = np.mean(np.square(y - output))\n",
    "            losses.append(loss)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6942370050265022\n",
      "Epoch 100, Loss: 0.7849214480874317\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(activation):\n",
    "    model = NeuralNetwork(input_size=X_train.shape[1], hidden_size=64, output_size=1, activation=activation)\n",
    "    losses = model.train(X_train, y_train.reshape(-1, 1), epochs=2000, learning_rate=0.01)\n",
    "    \n",
    "    train_accuracy = np.mean(model.predict(X_train) == y_train.reshape(-1, 1))\n",
    "    test_accuracy = np.mean(model.predict(X_test) == y_test.reshape(-1, 1))\n",
    "    \n",
    "    print(f'\\n{activation.capitalize()} Activation:')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    plt.plot(losses)\n",
    "    plt.title(f'Loss over Epochs ({activation})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate the model with different activation functions\n",
    "for activation in ['sigmoid', 'relu', 'tanh']:\n",
    "    train_and_evaluate(activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_and_loss(accuracies, losses, activation):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title(f'Loss over Epochs ({activation})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(f'Accuracy over Epochs ({activation})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
